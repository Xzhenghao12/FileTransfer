{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#import torchvision\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import mne\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(10, 1, 55) # CNN1\n",
    "        self.pool = nn.MaxPool2d(1, 16)\n",
    "        self.conv2 = nn.Conv2d(5, 1, 25) # CNN2\n",
    "        self.batch = nn.BatchNorm2d()\n",
    "        self.dense1 = nn.Linear(10) # dense\n",
    "        self.dense2 = nn.Linear(5) # dense\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(self.batch(F.relu(self.conv1(x)))) # CNN1\n",
    "        x = self.pool(self.batch(F.relu(self.conv2(x)))) # CNN2\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.dense1(x)) # relu activation\n",
    "        x = F.softmax(self.dense2(x)) # softmax activation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "A = np.array([1, 2, 3])\n",
    "B = np.array([1, 2, 4])\n",
    "A, B = torch.Tensor(A), torch.Tensor(B)\n",
    "print(torch.sum(A==B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 5])\n",
      "tensor([[0.6751, 0.6729, 0.8042, 0.1923, 0.0071],\n",
      "        [0.5162, 0.9818, 0.1838, 0.1339, 0.0855],\n",
      "        [0.5879, 0.9085, 0.5527, 0.0845, 0.6501],\n",
      "        [0.3729, 0.6553, 0.9589, 0.4340, 0.5202],\n",
      "        [0.4263, 0.7769, 0.1532, 0.5452, 0.7224],\n",
      "        [0.2034, 0.7723, 0.0827, 0.4716, 0.7123]])\n",
      "tensor([[0.6751, 0.6729, 0.8042, 0.1923, 0.0071],\n",
      "        [0.6751, 0.6729, 0.8042, 0.1923, 0.0071],\n",
      "        [0.5162, 0.9818, 0.1838, 0.1339, 0.0855],\n",
      "        [0.5879, 0.9085, 0.5527, 0.0845, 0.6501],\n",
      "        [0.3729, 0.6553, 0.9589, 0.4340, 0.5202],\n",
      "        [0.4263, 0.7769, 0.1532, 0.5452, 0.7224]])\n"
     ]
    }
   ],
   "source": [
    "## Debug function set_generate\n",
    "# first generate pseudo-training data\n",
    "def set_generate(x):  # input NxM signal\n",
    "    set_all = []\n",
    "    N, M = x.shape # M is the number of data points of each epoch, N is the number of epoch\n",
    "    x_0 = x[0, :] # data of the first epoch \n",
    "    x_f = x[-1, :] # data of the last epoch\n",
    "    x_current = x # current set\n",
    "    set_all.append(x_current)\n",
    "    x_previous = np.roll(x, 1, axis=0)\n",
    "    x_previous[0, :] = x_0  # ''previous'' set\n",
    "    set_all.append(x_previous)\n",
    "    x_next = np.roll(x, -1, axis=0)\n",
    "    x_next[0, :] = x_f # ''next'' set\n",
    "    set_all.append(x_next)\n",
    "    set_all = torch.Tensor(np.array(set_all))\n",
    "    x_present, x_previous, x_next = set_all\n",
    "    return x_present, x_previous, x_next\n",
    "x = np.random.rand(6, 5)\n",
    "x_present, x_previous, x_next = set_generate(x)\n",
    "\n",
    "#x = torch.Tensor(x)\n",
    "#y = np.array([1,2,3,4,2,1]).reshape(-1,1)\n",
    "#y = torch.Tensor(y)\n",
    "print(x_present.shape)\n",
    "print(x_present)\n",
    "print(x_previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([[[0.7914, 0.2609, 0.6006,  ..., 0.0307, 0.1315, 0.1159]]]), array([1])), (tensor([[[0.1312, 0.2037, 0.1271,  ..., 0.7571, 0.1158, 0.8011]]]), array([1])), (tensor([[[0.8774, 0.4772, 0.8136,  ..., 0.6337, 0.7330, 0.2619]]]), array([2])), (tensor([[[0.5954, 0.1356, 0.7059,  ..., 0.3260, 0.1988, 0.1966]]]), array([4])), (tensor([[[0.7287, 0.3694, 0.1642,  ..., 0.7542, 0.5326, 0.8044]]]), array([3])), (tensor([[[0.2945, 0.6379, 0.5975,  ..., 0.2747, 0.4902, 0.0801]]]), array([3]))]\n",
      "torch.Size([6, 5])\n"
     ]
    }
   ],
   "source": [
    "## Debug CNN model\n",
    "# prepare a 4 x 3000 dataset for debugging\n",
    "x_debug = torch.rand(6, 1, 1, 3000) \n",
    "# N, M = x_debug.shape\n",
    "events_train = np.array([1,1,2,4,3,3]).reshape(-1,1)\n",
    "y_debug = np.array([1,1,2,4,3,3]).reshape(-1,1)\n",
    "y_debug = torch.Tensor(events_train)\n",
    "train_data = list(zip(x_debug, events_train)) \n",
    "train_loader = DataLoader(train_data)\n",
    "print(train_data)\n",
    "m = nn.Conv2d(1, 10 , (1, 55), padding='same')\n",
    "output = m(x_debug)\n",
    "m = nn.BatchNorm2d(10)\n",
    "output = m(output)\n",
    "m = nn.MaxPool2d(kernel_size=(1, 16), ceil_mode=True)\n",
    "output = m(output)\n",
    "m = nn.Conv2d(10, 5 , (1, 25), padding='same')\n",
    "output = m(output)\n",
    "m = nn.MaxPool2d(kernel_size=(1, 16), ceil_mode=True)\n",
    "output = m(output)\n",
    "m = nn.Flatten()\n",
    "output = m(output)\n",
    "m = nn.Linear(output.shape[1], 10)\n",
    "output = m(output)\n",
    "output = F.relu(output)\n",
    "m = nn.Linear(10, 5)\n",
    "output = m(output)\n",
    "m = nn.Softmax(dim=0)\n",
    "output = m(output)\n",
    "print(output.shape)\n",
    "\n",
    "# from here, we get 5 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3982, 0.4006, 0.2288, 0.9587, 0.9813],\n",
      "        [0.8011, 0.1549, 0.3968, 0.9406, 0.8389],\n",
      "        [0.7264, 0.6093, 0.8447, 0.8673, 0.1460],\n",
      "        [0.1324, 0.2173, 0.2836, 0.6652, 0.4339],\n",
      "        [0.8869, 0.2896, 0.3612, 0.3382, 0.2279],\n",
      "        [0.1695, 0.9622, 0.0634, 0.8426, 0.6103],\n",
      "        [0.7545, 0.7025, 0.1083, 0.5020, 0.2746],\n",
      "        [0.3001, 0.7268, 0.6771, 0.7439, 0.9983]])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "y_debug = np.array([0,1,1,2,3,3,4,4])\n",
    "y_debug = torch.Tensor(y_debug)\n",
    "x_debug = torch.rand(8, 5)\n",
    "\n",
    "\n",
    "print(x_debug)\n",
    "#print(indx)\n",
    "#print(x_debug[indx_row, ind_col])\n",
    "loss = 0\n",
    "\n",
    "events_train = np.array([0,1,1,2,3,3,4,4])\n",
    "N = len(events_train)\n",
    "Nc = np.sum(events_train == 1)\n",
    "p1 = Nc/N\n",
    "p2 = 1 - p1\n",
    "W = np.zeros(N)\n",
    "W[:] = p2\n",
    "ind_corres = np.where(events_train == 1)\n",
    "W[ind_corres] = p1\n",
    "w = 0.5 / W\n",
    "w = torch.tensor(w)\n",
    "print(w.shape)\n",
    "\n",
    "\n",
    "indx_row = np.arange(N).reshape(-1, 1)\n",
    "indx_col = events_train.reshape(-1, 1)\n",
    "\n",
    "#for i in range(5): # class i\n",
    "#    indx_row = torch.nonzero(target.int() == i)\n",
    "#    indx_col = i\n",
    "#    w = self.weight\n",
    "#    w = w[indx_row]\n",
    "#    subloss = -torch.sum(torch.log(w * predictions[indx_row, indx_col]))\n",
    "#    loss = loss + subloss\n",
    "#    loss = loss / len(target)\n",
    "\n",
    "#for i in range(5):\n",
    "#    indx_row = torch.nonzero(y_debug.int() == i)\n",
    "#    indx_col = i\n",
    "#    weight = w[indx_row]\n",
    "#    subloss = -torch.sum(torch.log(weight * x_debug[indx_row, indx_col]))\n",
    "#    loss = loss + subloss\n",
    "\n",
    "#print(weight)\n",
    "#print(x_debug[indx_row, indx_col])\n",
    "#print(weight * x_debug[indx_row, indx_col])\n",
    "#print(loss)\n",
    "#loss = loss/len(y_debug)\n",
    "#print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 256])\n",
      "torch.Size([6, 5])\n"
     ]
    }
   ],
   "source": [
    "# Debug BiLSTM model\n",
    "## model:\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(75, 128, bidirectional=True)\n",
    "        self.dense = nn.Linear(256, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dense(x)\n",
    "        x = F.softmax(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "x_debug = torch.rand(6, 75)\n",
    "m = nn.LSTM(75, 128, bidirectional=True)\n",
    "output, _ = m(x_debug)\n",
    "print(output.shape)\n",
    "m = nn.Linear(256, 5)\n",
    "output = m(output)\n",
    "output = F.softmax(output, dim=1)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N1 sub-model ['N1' 'N2' 'N1' 'N2' 'N3'] : tensor([1.2500, 0.8333, 1.2500, 0.8333, 0.8333])\n",
      "N2 sub-model ['N1' 'N2' 'N1' 'N2' 'N3'] : tensor([0.8333, 1.2500, 0.8333, 1.2500, 0.8333])\n",
      "N3 sub-model ['N1' 'N2' 'N1' 'N2' 'N3'] : tensor([0.6250, 0.6250, 0.6250, 0.6250, 2.5000])\n"
     ]
    }
   ],
   "source": [
    "## Debug Weight for each submodels, take N1-N3 as example\n",
    "submodel_name = np.array(['N1', 'N2', 'N3'])\n",
    "labels_example = np.array(['N1', 'N2', 'N1', 'N2', 'N3']) # example labels with size = 5.\n",
    "N = len(labels_example)\n",
    "i = 0\n",
    "\n",
    "for stage in submodel_name:\n",
    "    Nc = np.sum(labels_example==stage)\n",
    "    p1 = Nc / N # probability of 'stage' in the corresponding model\n",
    "    p2 = 1 - p1 # probability of other 'stage' in this model\n",
    "    #p[:] = p2\n",
    "    #p[i] = p1\n",
    "    #w = 0.5 / p # weight for each stage in this sub-model\n",
    "    W = np.zeros(N) # initialize weight vector\n",
    "    W[:] = p2 \n",
    "    ind_corres = np.where(labels_example==stage) # get the index of stages of corresponding sub-model\n",
    "    W[ind_corres] = p1\n",
    "    W = 0.5 / W\n",
    "    W = torch.Tensor(W)\n",
    "    i = i+1\n",
    "    #print(stage, 'sub-model:', submodel_name, ':', w)\n",
    "    print(stage, 'sub-model', labels_example, ':', W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Not setting metadata\n",
      "841 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 841 events and 3000 original time points ...\n",
      "0 bad epochs dropped\n",
      "(841, 7, 3000)\n",
      "Using data from preloaded Raw for 841 events and 3000 original time points ...\n",
      "(841, 3000)\n",
      "(841, 1)\n",
      "<RawEDF | SC4001E0-PSG.edf, 7 x 7950000 (79500.0 s), ~424.6 MB, data loaded>\n",
      "(842, 3000)\n",
      "(842, 1)\n",
      "(841, 3000)\n",
      "(841, 1)\n"
     ]
    }
   ],
   "source": [
    "# Debug loading system\n",
    "import mne\n",
    "alice_files = [\"G:\\My Drive\\Sleep\\sleep-edf-database-expanded-1.0.0\\sleep-edf-database-expanded-1.0.0\\sleep-cassette\\SC4001E0-PSG.edf\",\n",
    "               \"G:\\My Drive\\Sleep\\sleep-edf-database-expanded-1.0.0\\sleep-edf-database-expanded-1.0.0\\sleep-cassette\\SC4001EC-Hypnogram.edf\"]\n",
    "raw_train = mne.io.read_raw_edf(\n",
    "    alice_files[0],\n",
    "    stim_channel=\"Event marker\",\n",
    "    infer_types=True,\n",
    "    preload=True,\n",
    "    verbose=\"error\",  # ignore issues with stored filter settings\n",
    ")\n",
    "annot_train = mne.read_annotations(alice_files[1])\n",
    "\n",
    "raw_train.set_annotations(annot_train, emit_warning=False)\n",
    "\n",
    "# plot some data\n",
    "# scalings were chosen manually to allow for simultaneous visualization of\n",
    "# different channel types in this specific dataset\n",
    "\n",
    "annotation_desc_2_event_id = {\n",
    "    \"Sleep stage W\": 1,\n",
    "    \"Sleep stage 1\": 2,\n",
    "    \"Sleep stage 2\": 3,\n",
    "    \"Sleep stage 3\": 4,\n",
    "    \"Sleep stage 4\": 4,\n",
    "    \"Sleep stage R\": 5,\n",
    "}\n",
    "\n",
    "# keep last 30-min wake events before sleep and first 30-min wake events after\n",
    "# sleep and redefine annotations on raw data\n",
    "annot_train.crop(annot_train[1][\"onset\"] - 30 * 60, annot_train[-2][\"onset\"] + 30 * 60)\n",
    "raw_train.set_annotations(annot_train, emit_warning=False)\n",
    "\n",
    "events_train, _ = mne.events_from_annotations(\n",
    "    raw_train, event_id=annotation_desc_2_event_id, chunk_duration=30.0\n",
    ")\n",
    "\n",
    "# create a new event_id that unifies stages 3 and 4\n",
    "event_id = {\n",
    "    \"Sleep stage W\": 1,\n",
    "    \"Sleep stage 1\": 2,\n",
    "    \"Sleep stage 2\": 3,\n",
    "    \"Sleep stage 3/4\": 4,\n",
    "    \"Sleep stage R\": 5,\n",
    "}\n",
    "\n",
    "tmax = 30.0 - 1.0 / raw_train.info[\"sfreq\"]  # tmax in included\n",
    "\n",
    "epochs = mne.Epochs(\n",
    "    raw=raw_train,\n",
    "    events=events_train,\n",
    "    event_id=event_id,\n",
    "    tmin=0.0,\n",
    "    tmax=tmax,\n",
    "    baseline=None,\n",
    ")\n",
    "# del raw_train\n",
    "epochs.info\n",
    "print(epochs.get_data().shape)\n",
    "epochs_train = epochs.get_data()[:,0,:]\n",
    "events_train = events_train[:, -1].reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'filepath-psg.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[167], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmne\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmne\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_raw_edf, concatenate_raws \n\u001b[1;32m----> 6\u001b[0m path_psg \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfilepath-psg.xlsx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m path_psg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39mravel(path_psg\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist()))\n\u001b[0;32m      9\u001b[0m path_hyp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(io\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilepath-hyp.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\xzhen\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    494\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 495\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\xzhen\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1550\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1551\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1554\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1555\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1557\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\xzhen\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[1;34m(content_or_path, storage_options)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1400\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[1;32m-> 1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[0;32m   1405\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   1406\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\xzhen\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'filepath-psg.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne.io import read_raw_edf, concatenate_raws \n",
    "\n",
    "path_psg = pd.read_excel(io='filepath-psg.xlsx', header=None)\n",
    "path_psg = list(np.ravel(path_psg.values.tolist()))\n",
    "\n",
    "path_hyp = pd.read_excel(io='filepath-hyp.xlsx', header=None)\n",
    "path_hyp = list(np.ravel(path_hyp.values.tolist()))\n",
    "\n",
    "\n",
    "#raws = [read_raw_edf(file, preload=True) for file in path_psg]\n",
    "#data = concatenate_raws(raws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5023, 0.1447, 0.8197, 0.4740, 0.9310],\n",
      "        [0.2122, 0.8518, 0.0427, 0.1589, 0.7650],\n",
      "        [0.2654, 0.1662, 0.3603, 0.1758, 0.7407],\n",
      "        [0.8739, 0.1780, 0.9209, 0.7400, 0.1299],\n",
      "        [0.7337, 0.9146, 0.3225, 0.7295, 0.3336],\n",
      "        [0.3338, 0.9430, 0.6573, 0.6252, 0.4321],\n",
      "        [0.9042, 0.6314, 0.8092, 0.1849, 0.3303],\n",
      "        [0.6423, 0.1659, 0.6461, 0.7879, 0.9842]])\n",
      "tensor([0., 0., 1., 1., 2., 2., 3., 4.])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32) tensor([0, 0, 1, 1, 2, 2, 3, 4], dtype=torch.int32)\n",
      "(tensor([0, 1, 2, 3, 4, 5, 6, 7], dtype=torch.int32), tensor([0, 0, 1, 1, 2, 2, 3, 4], dtype=torch.int32))\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([1.0046, 0.4243, 0.1108, 0.1186, 0.2150, 0.4382, 0.1233, 0.6561])\n"
     ]
    }
   ],
   "source": [
    "# Loss Function Revised Version \n",
    "import numpy as np\n",
    "import torch\n",
    "x_debug = torch.rand(8,5)\n",
    "N, c = x_debug.shape\n",
    "print(x_debug)\n",
    "label = np.array([0, 0, 1, 1, 2, 2, 3, 4])\n",
    "label = torch.Tensor(label)\n",
    "print(label)\n",
    "indx_row = torch.Tensor(np.arange(N)).int()\n",
    "indx_col = label.int()\n",
    "indx_features = (indx_row, indx_col)\n",
    "print(indx_features)\n",
    "predictions = x_debug[indx_features]\n",
    "w = torch.zeros(N)\n",
    "print(w)\n",
    "print(w * predictions)\n",
    "\n",
    "events_train = np.array([0, 0, 1, 1, 2, 2, 3, 4])\n",
    "Nc = np.sum(events_train == 0)\n",
    "N = len(events_train)\n",
    "p1 = Nc/N\n",
    "p2 = 1 - p1\n",
    "w = np.zeros(N)\n",
    "w[:] = p2\n",
    "ind_corres = np.where(events_train == 0)\n",
    "w[ind_corres] = p1\n",
    "w = torch.Tensor(0.5 / w)\n",
    "print(w * predictions)\n",
    "\n",
    "#print(zeros)\n",
    "\n",
    "#print(zeros)\n",
    "#indx_features = torch.nonzero(zeros == 1, as_tuple=True)\n",
    "#print(indx_features)\n",
    "#print(x_debug[indx_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3430, 0.3203, 0.3367],\n",
      "        [0.2359, 0.4366, 0.3275]])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "A = torch.rand(2,3)\n",
    "m = nn.Softmax(dim=1)\n",
    "A = m(A)\n",
    "print(A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
